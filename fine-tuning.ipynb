{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from openai import cli\n",
    "import os\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_version = '2022-12-01'\n",
    "openai.api_base = os.getenv(\"EAST_US_OPENAI_API_BASE\")\n",
    "openai.api_type = 'azure'\n",
    "openai.api_key = os.getenv(\"EAST_US_OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating the training file: training.jsonl\n",
      "Copying the training file to the validation file\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'validation.jsonl'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 학습, 검증 데이터 파일 생성 \n",
    "import shutil\n",
    "import json\n",
    "\n",
    "training_file_name = 'training.jsonl'\n",
    "validation_file_name = 'validation.jsonl'\n",
    "\n",
    "sample_data = [{\"prompt\": \"When I go to the store, I want an\", \"completion\": \"apple.\"},\n",
    "    {\"prompt\": \"When I go to work, I want a\", \"completion\": \"coffee.\"},\n",
    "    {\"prompt\": \"When I go home, I want a\", \"completion\": \"soda.\"}]\n",
    "\n",
    "print(f'Generating the training file: {training_file_name}')\n",
    "with open(training_file_name, 'w') as training_file:\n",
    "    for entry in sample_data:\n",
    "        json.dump(entry, training_file)\n",
    "        training_file.write('\\n')\n",
    "\n",
    "print(f'Copying the training file to the validation file')\n",
    "shutil.copy(training_file_name, validation_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # OpenAI에 학습 및 검증 파일이 이미 업로드 되어있는지 확인 \n",
    "# print('Checking for existing uploaded files.')\n",
    "# results = []\n",
    "# files = openai.File.list().data\n",
    "# print(f'Found {len(files)} total uploaded files in the subscription.')\n",
    "# for item in files:\n",
    "#     if item[\"filename\"] in [training_file_name, validation_file_name]:\n",
    "#         results.append(item[\"id\"])\n",
    "# print(f'Found {len(results)} already uploaded files that match our names.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f'Deleting already uploaded files...')\n",
    "# for id in results:\n",
    "#     openai.File.delete(sid = id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Upload progress: 100%|██████████| 204/204 [00:00<00:00, 190kit/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded file from training.jsonl: file-acf1d891d0d14344b236b1a4f2218ea6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Upload progress: 100%|██████████| 204/204 [00:00<?, ?it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded file from validation.jsonl: file-23d100935c184a3ab06ea2f63ecbd9cb\n",
      "Status (training_file | validation_file): notRunning | notRunning\n",
      "Status (training_file | validation_file): running | running\n",
      "Status (training_file | validation_file): running | running\n",
      "Status (training_file | validation_file): running | running\n",
      "Status (training_file | validation_file): succeeded | running\n",
      "Status (training_file | validation_file): succeeded | succeeded\n"
     ]
    }
   ],
   "source": [
    "# OpenAI에 파일 업로드 및 업로드 상태 체크 \n",
    "import time\n",
    "\n",
    "def check_status(training_id, validation_id):\n",
    "    train_status = openai.File.retrieve(training_id)[\"status\"]\n",
    "    valid_status = openai.File.retrieve(validation_id)[\"status\"]\n",
    "    print(f'Status (training_file | validation_file): {train_status} | {valid_status}')\n",
    "    return (train_status, valid_status)\n",
    "\n",
    "#importing our two files\n",
    "training_id = cli.FineTune._get_or_upload(training_file_name, True)\n",
    "validation_id = cli.FineTune._get_or_upload(validation_file_name, True)\n",
    "\n",
    "#checking the status of the imports\n",
    "(train_status, valid_status) = check_status(training_id, validation_id)\n",
    "\n",
    "while train_status not in [\"succeeded\", \"failed\"] or valid_status not in [\"succeeded\", \"failed\"]:\n",
    "    time.sleep(1)\n",
    "    (train_status, valid_status) = check_status(training_id, validation_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "print(f'Downloading training file: {training_id}')\n",
    "result = openai.File.download(training_id)\n",
    "print(result.decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_args = {\n",
    "    \"training_file\": training_id,\n",
    "    \"validation_file\": validation_id,\n",
    "    \"model\": \"babbage\",\n",
    "    \"compute_classification_metrics\": True,\n",
    "    \"classification_n_classes\": 3,\n",
    "    \"n_epochs\": 20,\n",
    "    \"batch_size\": 3,\n",
    "    \"learning_rate_multiplier\": 0.3\n",
    "}\n",
    "resp = openai.FineTune.create(**create_args)\n",
    "job_id = resp[\"id\"]\n",
    "status = resp[\"status\"]\n",
    "\n",
    "print(f'Fine-tunning model with jobID: {job_id}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import signal\n",
    "import datetime\n",
    "\n",
    "def signal_handler(sig, frame):\n",
    "    status = openai.FineTune.retrieve(job_id).status\n",
    "    print(f\"Stream interrupted. Job is still {status}.\")\n",
    "    return\n",
    "\n",
    "print(f'Streaming events for the fine-tuning job: {job_id}')\n",
    "signal.signal(signal.SIGINT, signal_handler)\n",
    "\n",
    "events = openai.FineTune.stream_events(job_id)\n",
    "try:\n",
    "    for event in events:\n",
    "        print(f'{datetime.datetime.fromtimestamp(event[\"created_at\"])} {event[\"message\"]}')\n",
    "\n",
    "except Exception:\n",
    "    print(\"Stream interrupted (client disconnected).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status = openai.FineTune.retrieve(id=job_id)[\"status\"]\n",
    "if status not in [\"succeeded\", \"failed\"]:\n",
    "    print(f'Job not in terminal status: {status}. Waiting.')\n",
    "    while status not in [\"succeeded\", \"failed\"]:\n",
    "        time.sleep(2)\n",
    "        status = openai.FineTune.retrieve(id=job_id)[\"status\"]\n",
    "        print(f'Status: {status}')\n",
    "else:\n",
    "    print(f'Finetune job {job_id} finished with status: {status}')\n",
    "\n",
    "print('Checking other finetune jobs in the subscription.')\n",
    "result = openai.FineTune.list()\n",
    "print(f'Found {len(result.data)} finetune jobs.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fist let's get the model of the previous job:\n",
    "result = openai.FineTune.retrieve(id=job_id)\n",
    "if result[\"status\"] == 'succeeded':\n",
    "    model = result[\"fine_tuned_model\"]\n",
    "\n",
    "# Now let's create the deployment\n",
    "print(f'Creating a new deployment with model: {model}')\n",
    "result = openai.Deployment.create(model=model, scale_settings={\"scale_type\":\"standard\"})\n",
    "deployment_id = result[\"id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Checking for deployment status.')\n",
    "resp = openai.Deployment.retrieve(id=deployment_id)\n",
    "status = resp[\"status\"]\n",
    "print(f'Deployment {deployment_id} is with status: {status}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('While deployment running, selecting a completed one.')\n",
    "deployment_id = None\n",
    "result = openai.Deployment.list()\n",
    "for deployment in result.data:\n",
    "    if deployment[\"status\"] == \"succeeded\":\n",
    "        deployment_id = deployment[\"id\"]\n",
    "        break\n",
    "\n",
    "if not deployment_id:\n",
    "    print('No deployment with status: succeeded found.')\n",
    "else:\n",
    "    print(f'Found a successful deployment with id: {deployment_id}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Sending a test completion job')\n",
    "start_phrase = 'When I go home, I want a'\n",
    "response = openai.Completion.create(deployment_id=deployment_id, prompt=start_phrase, temperature=0, stop=\".\")\n",
    "text = response['choices'][0]['text'].replace('\\n', '').replace(' .', '.').strip()\n",
    "print(f'\"{start_phrase} {text}.\"')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
